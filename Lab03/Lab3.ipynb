{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: LLMs, Prompting and RAG\n",
    "\n",
    "## June 13, 2024\n",
    "\n",
    "Welcome to Lab 3 of our course on Natural Language Processing. Today, we will be diving deep into the fourth and most recent paradigm in NLP teased in the previous Lab, i.e. Pre-train, Prompt and Predict. The core idea behind the paradigm is that once we train a big enough language model (pre-training + instruction tuning), we do not really need to train these models further to solve any specific taks, but instead can directly prompt the model to solve a task by specifying instructions, task descriptions and in some cases a few examples.\n",
    "\n",
    "Like last time we will be working on the with the [SocialIQA](https://arxiv.org/abs/1904.09728) dataset, and demonstrating how to work with LLMs to solve such tasks.\n",
    "\n",
    "Along with the SocialIQA dataset, we will also delve into the fascinating world of Retrieval Augmented Generation (RAG). RAG is a powerful technique that combines the strengths of pre-trained language models and information retrieval systems to generate contextually relevant responses.\n",
    "\n",
    "For building the RAG system, why not build something which might be useful for plaksha students. We will build a Question Answering system which will be able to answer questions about the Plaksha Professors. For this task, the dataset is already generated by scraping the plaksha full-time faculty page to get information about various professors, their areas of expertise, research interests, and more. Our goal is to convert this data into embeddings.\n",
    "\n",
    "Once we have these embeddings, we can use them to retrieve contextually relevant information based on a given query. For instance, if a student wants to know which professor specializes in Natural Language Processing, our RAG system should be able to retrieve the relevant professor details.\n",
    "\n",
    "The final part of our system is a Language Model (LM). Once we have the relevant context from our retrieval system, we pass it to a pre-trained LM. The LM then generates a coherent and contextually appropriate response.\n",
    "\n",
    "By the end of this lab, you will have a hands-on understanding of how to build a RAG system. You will learn how to convert text data into embeddings, how to retrieve relevant context based on a query, and how to generate responses using a pre-trained LM.\n",
    "\n",
    "This Lab doesn't require any GPU, since we will be heavily using APIs from various third party sources.  \n",
    "\n",
    "For the embeddings we will be utilizing [Voyage AI's latest Embedding model](https://docs.voyageai.com/docs/embeddings) via their API. The api provdies free access to embeddings upto 50 Million tokens, which are plenty for our assignments and even for your final projects if needed.  \n",
    "*Note: The Voyage API has very low rate limits when you don't add payment details, with 3 RPM(requests per minute)*\n",
    "\n",
    "For Language Models, we have two options, the first one being [groq](https://console.groq.com/docs/models), which has various models like LLaMa 3 8b/70b, Mixtral 8x7b and Gemma 7b which are free to use and have very high throughput.  \n",
    "Another option is to use [Open Router](https://openrouter.ai/docs/models), where there are plenty of free model options available.  \n",
    "Both service providers are compatible with OpenAI package, and hence can be used interchangebly by just changing `base_url`, `api_key` and `model_name`.\n",
    "\n",
    "Learning Outcomes of the Lab:\n",
    "- **Mastering Prompting Techniques:** Learn how to effectively prompt large language models to solve specific tasks and to work with the SocialIQA dataset, demonstrating the practical use of LLMs in solving real-world NLP tasks.\n",
    "- **Embedding Creation and Utilization:** Gain hands-on experience in converting text data into embeddings using embedding model and utilizing these embeddings for information retrieval.\n",
    "- **Understanding Retrieval-Augmented Generation (RAG):** Learn how to combine pre-trained language models with information retrieval systems to generate contextually relevant responses.\n",
    "\n",
    "Recommended Reading:\n",
    "- Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig. <i>Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</i>. https://arxiv.org/abs/2107.13586\n",
    "\n",
    "- Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang. <i>Retrieval-Augmented Generation for Large Language Models: A Survey</i>. https://arxiv.org/abs/2312.10997\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U voyageai\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "siqa_data_dir = \"gdrive/MyDrive/PlakshaNLP2024/Lab3/data/socialiqa-train-dev/\"\n",
    "plaksha_data_dir = \"gdrive/MyDrive/PlakshaNLP2024/Lab3/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing libraries that we will be making use of in the assignment.\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "import openai\n",
    "import voyageai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = \"\" # Your OpenRouter API Key: https://openrouter.ai/keys\n",
    "client = openai.OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=OPENROUTER_API_KEY\n",
    ")\n",
    "\n",
    "VOYAGE_API_KEY = \"\" # Your Voyage API Key: https://dash.voyageai.com/api-keys\n",
    "vo = voyageai.Client(VOYAGE_API_KEY)\n",
    "\n",
    "GROQ_API_KEY = \"\" # Your GROQ API Key: https://console.groq.com/keys\n",
    "client = openai.OpenAI(\n",
    "  base_url=\"https://api.groq.com/openai/v1\",\n",
    "  api_key=GROQ_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the SocialIQA dataset\n",
    "\n",
    "def load_siqa_data(split):\n",
    "\n",
    "    # We first load the file containing context, question and answers\n",
    "    with open(f\"data/socialiqa-train-dev/{split}.jsonl\") as f:\n",
    "        data = [json.loads(jline) for jline in f.read().splitlines()]\n",
    "\n",
    "    # We then load the file containing the correct answer for each question\n",
    "    with open(f\"data/socialiqa-train-dev/{split}-labels.lst\") as f:\n",
    "        labels = f.read().splitlines()\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "train_data, train_labels = load_siqa_data(\"train\")\n",
    "dev_data, dev_labels = load_siqa_data(\"dev\")\n",
    "\n",
    "print(f\"Number of Training Examples: {len(train_data)}\")\n",
    "print(f\"Number of Validation Examples: {len(dev_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Prompting Basics (30 minutes)\n",
    "\n",
    "In this task, you will be learning how create standard NLP problems into text prompts which can then be fed to an LLM for its prediction. Mainly there are 2 concepts that are important to understand while creating prompts:\n",
    "- Prompt Template or Function: a textual string that has two slots: an input slot [X] for input x and an answer slot\n",
    "[Z] for an intermediate generated answer text z that will later be mapped into y.\n",
    "- Answer verbalizer: A mapping between the task labels to words or phrases that converts the more artificial looking labels to natural language that fits with the prompt. eg. for sentiment analysis we can define Z = {“excellent”, “good”, “OK”, “bad”, “horrible”} to represent each of the classes in Y = {++, +, ~, -, --}.\n",
    "\n",
    "<img src=\"data/prompting_basics.png\" alt=\"prompting\" border=\"0\">\n",
    "\n",
    "We can also include more interesting stuff like instruction of the task in the template and explanation of the answer in the verbalizer to make more powerful prompts, as we will see a bit later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1 Defining prompt function and verbalizer for SocialIQA.\n",
    "\n",
    "For the purpose of this excercise, we ask you to implement this prompt function:\n",
    "```\n",
    "Context: {{context}}\n",
    "    Question: {{question}}\n",
    "    Which one of these answers best answers the question according to the context?\n",
    "    AnswerA: {{answerA}}\n",
    "    AnswerB: {{answerB}}\n",
    "    AnswerC: {{answerC}}\n",
    "```\n",
    "\n",
    "and verbalizer:\n",
    "\n",
    "```\n",
    "{\"1\": \"The answer is A\", \"2\": \"The answer is B\", \"3\": \"The answer is C\"}\n",
    "```\n",
    "\n",
    "This prompt was obtained from [PromptSource](https://huggingface.co/spaces/bigscience/promptsource), an awesome resource for finding prompts for hundreds of NLP tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def social_iqa_prompting_fn(siqa_example: dict[str, str]):\n",
    "    \"\"\"\n",
    "    Takes an example from the SocialIQA dataset, fills in the prompt template, and returns the prompt.\n",
    "    \n",
    "    Inputs:\n",
    "        siqa_example: A dictionary containing the context, question and answerA, answerB, answerC for a SocialIQA example.\n",
    "\n",
    "    Outputs:\n",
    "\n",
    "    \"\"\"\n",
    "    prompt = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Test Case 1\n",
    "print(\"Running Sample Test Case 1\")\n",
    "siqa_example = train_data[0]\n",
    "prompt = social_iqa_prompting_fn(siqa_example)\n",
    "expected_prompt = \"\"\"Context: Cameron decided to have a barbecue and gathered her friends together.\n",
    "    Question: How would Others feel as a result?\n",
    "    Which one of these answers best answers the question according to the context?\n",
    "    AnswerA: like attending\n",
    "    AnswerB: like staying home\n",
    "    AnswerC: a good friend to have\"\"\"\n",
    "print(f\"Input Example:\\n{siqa_example}\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(f\"Expected Prompt:\\n{expected_prompt}\")\n",
    "assert prompt == expected_prompt\n",
    "\n",
    "# Sample Test Case 2\n",
    "print(\"Running Sample Test Case 2\")\n",
    "siqa_example = train_data[100]\n",
    "prompt = social_iqa_prompting_fn(siqa_example)\n",
    "expected_prompt = \"\"\"Context: Jordan's dog peed on the couch they were selling and Jordan removed the odor as soon as possible.\n",
    "    Question: How would Jordan feel afterwards?\n",
    "    Which one of these answers best answers the question according to the context?\n",
    "    AnswerA: selling a couch\n",
    "    AnswerB: Disgusted\n",
    "    AnswerC: Relieved\"\"\"\n",
    "print(f\"Input Example:\\n{siqa_example}\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(f\"Expected Prompt:\\n{expected_prompt}\")\n",
    "assert prompt == expected_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def social_iqa_verbalizer(label: str):\n",
    "    \"\"\"\n",
    "    Takes in the label and coverts it into a natural language phrase as specified above\n",
    "\n",
    "    Inputs:\n",
    "        label: A string containing the correct answer for a SocialIQA example.\n",
    "    \n",
    "    Outputs:\n",
    "        A string containing the natural language phrase corresponding to the label.\n",
    "    \"\"\"\n",
    "\n",
    "    verbalized_label = None\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return verbalized_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Test Case 1\n",
    "print(\"Running Sample Test Case 1\")\n",
    "siqa_example = train_labels[0]\n",
    "output = social_iqa_verbalizer(siqa_example)\n",
    "expected_output = \"\"\"The answer is A\"\"\"\n",
    "print(f\"Input Example:\\n{siqa_example}\")\n",
    "print(f\"output:\\n{output}\")\n",
    "print(f\"Expected output:\\n{expected_output}\")\n",
    "assert output == expected_output\n",
    "\n",
    "# Sample Test Case 2\n",
    "print(\"\\nRunning Sample Test Case 2\")\n",
    "siqa_example = train_labels[100]\n",
    "output = social_iqa_verbalizer(siqa_example)\n",
    "expected_output = \"\"\"The answer is B\"\"\"\n",
    "print(f\"Input Example:\\n{siqa_example}\")\n",
    "print(f\"output:\\n{output}\")\n",
    "print(f\"Expected output:\\n{expected_output}\")\n",
    "assert output == expected_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now obtain the prompts and verbalized labels for each of the the examples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompts = None\n",
    "train_verbalized_labels = None\n",
    "val_prompts = None\n",
    "val_verbalized_labels = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "train_prompts = [social_iqa_prompting_fn(example) for example in train_data]\n",
    "train_verbalized_labels = [social_iqa_verbalizer(label) for label in train_labels]\n",
    "val_prompts = [social_iqa_prompting_fn(example) for example in dev_data]\n",
    "val_verbalized_labels = [social_iqa_verbalizer(label) for label in dev_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Test Case 1\n",
    "print(\"Running Sample Test Case 1\")\n",
    "idx = 10\n",
    "siqa_example = train_data[idx]\n",
    "prompt = train_prompts[idx]\n",
    "expected_prompt = \"\"\"Context: Sydney was a school teacher and made sure their students learned well.\n",
    "    Question: How would you describe Sydney?\n",
    "    Which one of these answers best answers the question according to the context?\n",
    "    AnswerA: As someone that asked for a job\n",
    "    AnswerB: As someone that takes teaching seriously\n",
    "    AnswerC: Like a leader\"\"\"\n",
    "print(f\"Input Example:\\n{siqa_example}\")\n",
    "print(f\"Prompt:\\n{prompt}\")\n",
    "print(f\"Expected Prompt:\\n{expected_prompt}\")\n",
    "assert prompt == expected_prompt\n",
    "\n",
    "# Sample Test Case 2\n",
    "print(\"\\nRunning Sample Test Case 2\")\n",
    "idx = 10\n",
    "siqa_label = train_labels[idx]\n",
    "output = social_iqa_verbalizer(siqa_label)\n",
    "verbalized_label = \"The answer is B\"\n",
    "print(f\"Input Example:\\n{siqa_label}\")\n",
    "print(f\"Verbalized Label:\\n{verbalized_label}\")\n",
    "print(f\"Expected Verbalized Label:\\n{verbalized_label}\")\n",
    "assert output == verbalized_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to have a reverse verbalizer as well that converts the verbalized labels back to the structured and consistent labels in the dataset. For example, \"The answer is A\" is mapped back to \"1\" and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def social_iqa_reverse_verbalizer(verbalized_label: str):\n",
    "    \"\"\"\n",
    "    Reverses the verbalized label into the label\n",
    "    Inputs:\n",
    "        verbalized_label: A string containing the natural language phrase corresponding to the label.\n",
    "    Outputs:\n",
    "        label: A string containing the correct answer for a SocialIQA example.\n",
    "    \n",
    "    Important Note: We will be using this function to map LLM's output to structured label. The output of LLM now can be in some format other than what we expect\n",
    "    For example, it can be \"The answer is A\" or \"The answer is A.\" or or \"<some text> answer is A\" or \"The answer is A <some text>\"\n",
    "    When you reverse the verbalized label, make sure you handle these cases.\n",
    "    \n",
    "    Important Note 2: If the resulting text doesn't have the answer, then just return an empty string.\n",
    "\n",
    "    HINT: use regex pattern matching.\n",
    "    \"\"\"\n",
    "\n",
    "    label = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Test Case 1\n",
    "print(\"Running Sample Test Case 1\")\n",
    "example_verbalized_label = \"The answer is C\"\n",
    "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
    "expected_output = \"3\"\n",
    "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
    "print(f\"output:\\n{output}\")\n",
    "print(f\"Expected output:\\n{expected_output}\")\n",
    "assert output == expected_output\n",
    "\n",
    "# Sample Test Case 2\n",
    "print(\"\\nRunning Sample Test Case 2\")\n",
    "example_verbalized_label = \"The answer is B\"\n",
    "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
    "expected_output = \"2\"\n",
    "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
    "print(f\"output:\\n{output}\")\n",
    "print(f\"Expected output:\\n{expected_output}\")\n",
    "assert output == expected_output\n",
    "\n",
    "# Sample Test Case 3\n",
    "print(\"\\nRunning Sample Test Case 3\")\n",
    "example_verbalized_label = \"some explanation before the actual answer, The answer is A\"\n",
    "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
    "expected_output = \"1\"\n",
    "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
    "print(f\"output:\\n{output}\")\n",
    "print(f\"Expected output:\\n{expected_output}\")\n",
    "assert output == expected_output\n",
    "\n",
    "# Sample Test Case 4\n",
    "print(\"\\nRunning Sample Test Case 4\")\n",
    "example_verbalized_label = \"some text here the answer is C, some more text\"\n",
    "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
    "expected_output = \"3\"\n",
    "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
    "print(f\"output:\\n{output}\")\n",
    "print(f\"Expected output:\\n{expected_output}\")\n",
    "assert output == expected_output\n",
    "\n",
    "# Sample Test Case 5\n",
    "print(\"\\nRunning Sample Test Case 5\")\n",
    "example_verbalized_label = \"none of the options is the correct answer\"\n",
    "output = social_iqa_reverse_verbalizer(example_verbalized_label)\n",
    "expected_output = \"\"\n",
    "print(f\"Input Example:\\n{example_verbalized_label}\")\n",
    "print(f\"output:\\n{output}\")\n",
    "print(f\"Expected output:\\n{expected_output}\")\n",
    "assert output == expected_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2: Choose Few-Shot examples\n",
    "\n",
    "Often we can get better performance on a task by providing a few examples of the task as part of the prompt. This is also known as in-context learning, where the model learns to solve a task based on the examples provided in the context (and no updates to the model's weights!). One of the easiest way that works reasonably well in practice is to simply choose `k` examples randomly for each class from the entire training dataset, such that we have n_classes * k few-shot examples where n_classes = 3 for SocialIQA dataset. Implement the `choose_few_shot` function below that does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_few_shot(train_prompts, train_verbalized_labels, k = 1, seed = 42):\n",
    "    \"\"\"\n",
    "    Randomly chooses k examples from the training set for few-shot in-context learning.\n",
    "    Inputs:\n",
    "        train_prompts: A list of prompts for the training set.\n",
    "        train_verbalized_labels: A list of labels for the training set.\n",
    "        k: The number of examples per class to choose.\n",
    "        n_classes: The number of classes in the dataset.\n",
    "        seed: The random seed to use, to ensure reproducible outputs\n",
    "\n",
    "    Outputs:\n",
    "        - List[Dict[str, str]]: A list of 3k examples from the training set, where each example is represented as a dictionary with \"prompt\" and \"label\" as keys and corresponding values.\n",
    "\n",
    "    Example Output: [\n",
    "        {\n",
    "            \"prompt\": <Example Prompt 1>,\n",
    "            \"label\": <Example Label_1>\n",
    "        },\n",
    "        ...,\n",
    "        {\n",
    "            \"prompt\": <Example Prompt 3k>,\n",
    "            \"label\": <Example Label_3k>\n",
    "        }\n",
    "    ]\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    fs_examples = []\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Shuffle the examples to ensure there is no bias in the order of the examples\n",
    "    random.shuffle(fs_examples)\n",
    "\n",
    "    return fs_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Test Case 1\n",
    "print(\"Running Sample Test Case 1. Checking if the output length is correct\")\n",
    "k = 1\n",
    "seed = 42\n",
    "output = choose_few_shot(train_prompts, train_verbalized_labels, k, seed)\n",
    "output_len = len(output)\n",
    "expected_output_len = k * len(set(train_labels))\n",
    "print(f\"k: {k}\")\n",
    "print(f\"Output Length:\\n{output_len}\")\n",
    "print(f\"Expected Output Length:\\n{expected_output_len}\")\n",
    "assert output_len == expected_output_len\n",
    "\n",
    "# Sample Test Case 2\n",
    "print(\"\\nRunning Sample Test Case 2. Checking if all labels are predicted\")\n",
    "output_labels = sorted(list(set([example[\"label\"] for example in output])))\n",
    "expected_output_labels = [\"The answer is A\", \"The answer is B\", \"The answer is C\"]\n",
    "print(f\"Output Labels:\\n{output_labels}\")\n",
    "print(f\"Expected Output Labels:\\n{expected_output_labels}\")\n",
    "assert output_labels == expected_output_labels\n",
    "\n",
    "# Sample Test Case 3\n",
    "print(\"\\nRunning Sample Test Case 3. Checking if count of labels are correct\")\n",
    "k = 3\n",
    "output = choose_few_shot(train_prompts, train_verbalized_labels, k, seed)\n",
    "output_label_counter = Counter(list(([example[\"label\"] for example in output])))\n",
    "expected_output_counter = {\"The answer is A\": k, \"The answer is B\": k, \"The answer is C\": k}\n",
    "print(f\"For k = {k}\")\n",
    "print(f\"Output Label Counter:\\n{output_label_counter}\")\n",
    "print(f\"Expected Output Label Counter:\\n{expected_output_counter}\")\n",
    "assert output_label_counter == expected_output_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 3 few-shot examples from training data\n",
    "few_shot_examples = choose_few_shot(train_prompts, train_verbalized_labels, k = 1, seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot examples with explanations\n",
    "\n",
    "So far above we have been constructing label verbalizer to provide the answer directly. Often it can be useful to prompt the model to first generate an explanation before the answer. For eg.\n",
    "```\n",
    "\"prompt\": \"Context: Tracy didn't go home that evening and resisted Riley's attacks.\n",
    "            Question: What does Tracy need to do before this?\n",
    "            Options: \n",
    "            (A) make a new plan \n",
    "            (B) Go home and see Riley \n",
    "            (C) Find somewhere to go\"\n",
    "\"label\": \"Tracy found somewhere to go and didn't come home because she wanted to resist Riley's attacks. Hence, the answer is C\"\n",
    "```\n",
    "One way to prompt the model to generate such explanations is to provide the explanations for the few-shot examples, which will ground the model to first generate an explanation and then the answer. This helps both improve the performance of the model as well as have more interpretable outputs from LLM.\n",
    "\n",
    "Below we provide a few examples with explanations for SocialIQA task obtained from [Super-NaturalInstructions](https://aclanthology.org/2022.emnlp-main.340/), an amazing resource for prompts, instructions and explanations for around 1600 NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_examples_w_explanations = [\n",
    "    {\n",
    "        \"prompt\": \"Context: Tracy didn't go home that evening and resisted Riley's attacks.\\nQuestion: What does Tracy need to do before this?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: make a new plan\\nAnswerB: Go home and see Riley\\AnswerC: Find somewhere to go\",\n",
    "        \"label\": \"Tracy found somewhere to go and didn't come home because she wanted to resist Riley's attacks. Hence, the correct answer is C.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Context: Sydney walked past a homeless woman asking for change but did not have any money they could give to her. Sydney felt bad afterwards.\\nQuestion: How would you describe Sydney?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: sympathetic\\nAnswerB: like a person who was unable to help\\nAnswerC: incredulous\",\n",
    "        \"label\": \"Sydney is a sympathetic person because she felt bad for someone who needed help, and she couldn't help her. Hence, the correct answer is A.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Context: Taylor gave help to a friend who was having trouble keeping up with their bills.\\nQuestion: What will their friend want to do next?\\nWhich one of these answers best answers the question according to the context?\\nAnswerA: help the friend find a higher paying job\\nAnswerB: thank Taylor for the generosity\\nAnswerC: pay some of their late employees\",\n",
    "        \"label\": \"The friend should thank Taylor for the generosity she showed by helping him pay bills. Hence, the correct answer is B.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_examples_w_explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Evaluating ChatGPT (GPT-3.5-Turbo) on SocialIQA (45 minutes)\n",
    "\n",
    "Today we will be working with OpenAI's GPT family of models. ChatGPT (or GPT-3.5) was built on top of GPT-3, which is a pre-trained Large Language Model (LLM) with 175 Billion parameters, trained on a huge amount of unlabelled data using the language modelling objective (i.e. given k tokens, generate (k+1)th token). While this forms the basis of all GPT family of models, GPT-3.5 and later models are based on [InstructGPT](https://arxiv.org/abs/2203.02155), which further adds an Instruction Tuning step that learns from human feedback to follow provided instructions.\n",
    "\n",
    "![Instruction Tuning](data/instructgpt.png)\n",
    "*From the [Ouyang et al. 2022](https://arxiv.org/abs/2203.02155)*\n",
    "\n",
    "As a consequence of the pre-training with language modeling objective and instruction tuning, we can use GPT-3.5 to complete a given piece of text and provide specific instructions about how to go about completing the text. We achieve this by defining a text prompt which is to be given as the input to the LLM which then generates a completion of the provided text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=\"llama3-70b-8192\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    ],\n",
    "  max_tokens=20,\n",
    "  temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to wrap our head around different parameters to this function call.\n",
    "\n",
    "First we have `model`, where we specify which OpenAI model to use. We have used `\"gpt-3.5-turbo\"` here, which is similar to ChatGPT like you would have used online. You can find the list of other models [here](https://platform.openai.com/docs/models).\n",
    "\n",
    "Next, we have `messages`, which contains the conversation between the user and assistant that is to be completed. Notice that the first message is what we call a \"system prompt\", which is used to set the behavior of the assistant. \n",
    "\n",
    "`max_tokens` is used to specify the maximum number of response tokens that the model should generate. This can be useful when you know how long the response is typically going to be, and can help reduce cost.\n",
    "\n",
    "`temperature`, helps in controlling the variability in the output. Lower values for temperature result in more consistent outputs, while higher values generate more diverse and creative results. Setting temperature to 0 will make the outputs mostly deterministic, but a small amount of variability will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the response. The assistant's reply can be  extracted with `response['choices'][0]['message']['content']`. Every response will include a finish_reason. The possible values for finish_reason are:\n",
    "\n",
    "- stop: API returned complete message, or a message terminated by one of the stop sequences provided via the stop parameter\n",
    "- length: Incomplete model output due to max_tokens parameter or token limit\n",
    "- function_call: The model decided to call a function\n",
    "- content_filter: Omitted content due to a flag from our content filters\n",
    "- null: API response still in progress or incomplete\n",
    "\n",
    "Depending on input parameters (like providing functions as shown below), the model response may include different information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = response.choices[0].message.content\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: Using ChatGPT to solve SocialIQA problems\n",
    "\n",
    "Now we have an understanding of how to work with OpenAI API, we can go ahead and call the api with the prompts that we just created and check how well does the model perform the task. We promt the model with the test example for which we want the prediction and provide few-shot examples as part of the context. This can be done by simply providing the example prompt and labels as user-assistant conversation history and test example as the most recent query of the user. Implement the function `get_social_iqa_pred_gpt` that receives a test prompt to be answered, few-shot examples, and some api specific hyperparameters to predict the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_social_iqa_pred_gpt(\n",
    "        test_prompt,\n",
    "        few_shot_examples,\n",
    "        model_name = \"llama3-70b-8192\",\n",
    "        max_tokens = 20,\n",
    "        temperature = 0.0,\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calls the OpenAI API with test_prompt and few-shot examples to generate the answer.\n",
    "    Inputs:\n",
    "        test_prompt: The prompt for the test example\n",
    "        few_shot_examples: A list of few-shot examples\n",
    "        model_name: The name of the model to use\n",
    "        max_tokens: The maximum number of tokens to generate\n",
    "        temperature: The temperature to use for the model\n",
    "\n",
    "    Outputs:\n",
    "        model_output: The model's output\n",
    "\n",
    "    Hint: Your messages to be sent should be in the following format:\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": <fs-example-1-promot>},\n",
    "            {\"role\": \"assistant\", \"content\": <fs-example-1-label>},\n",
    "            ...,\n",
    "            {\"role\": \"user\", \"content\": <fs-example-3k-promot>},\n",
    "            {\"role\": \"assistant\", \"content\": <fs-example-3k-label>},\n",
    "            {\"role\": \"user\", \"content\": <test-prompt>},\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    messages_prompt = [{\n",
    "        \"role\": \"user\", \"content\": \"You are an expert of Human Social Common Sense. You need to solve the SocialIQA task. In this task, you're given a context, a question, and three options. Your task is to find the correct answer to the question using the given context and options. Also, you may need to use commonsense reasoning about social situations to answer the questions. Classify your answers into 'A', 'B', and 'C'. You must choose the most likely option.\"\n",
    "    }]\n",
    "    model_output = None\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "            raise NotImplementedError()\n",
    "            break\n",
    "        except (openai.APIConnectionError, openai.RateLimitError, openai.Timeout, openai.InternalServerError) as e:\n",
    "            #Sleep and try again\n",
    "            print(f\"Couldn't get response due to {e}. Trying again!\")\n",
    "            # time.sleep(20)\n",
    "            continue\n",
    "\n",
    "    return model_output.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = val_prompts[0]\n",
    "test_example_label = val_verbalized_labels[0]\n",
    "model_output = get_social_iqa_pred_gpt(test_example, few_shot_examples,\n",
    "                                       model_name = \"llama3-70b-8192\",\n",
    "                                       max_tokens = 20, temperature = 0.0)\n",
    "print(test_example)\n",
    "print(f\"Model's response: \", model_output)\n",
    "print(f\"Correct answer: \", test_example_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the model didn't quite get the answer right. Let's try providing examples with explanations i.e. `fs_examples_w_explanations` and see the output. Note that we will need to give a higher value of `max_tokens`, since the model is also expected to generate explanation now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = val_prompts[0]\n",
    "test_example_label = val_verbalized_labels[0]\n",
    "model_output = get_social_iqa_pred_gpt(test_example, fs_examples_w_explanations,\n",
    "                                        model_name = \"llama3-70b-8192\",\n",
    "                                        max_tokens = 50, temperature = 0.0)\n",
    "print(test_example)\n",
    "print(f\"Model's response: \", model_output)\n",
    "print(f\"Correct answer: \", test_example_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output is correct and the explanation also makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a full fledged evaluation now. Due to API limits, we will only be evaluating first 32 examples of the validation set and not the whole but that should give us some idea of how good our LLM (LLaMa3-70b) is at solving social common-sense reasoning problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(\n",
    "        test_prompts,\n",
    "        few_shot_examples,\n",
    "        model_name = \"llama3-70b-8192\",\n",
    "        max_tokens = 20,\n",
    "        temperature = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Get predictions for all test prompts using the `get_social_iqa_pred_gpt` function\n",
    "\n",
    "    Inputs:\n",
    "        test_prompts: A list of test prompts\n",
    "        few_shot_examples: A list of few-shot examples\n",
    "        model_name: The name of the model to use\n",
    "        max_tokens: The maximum number of tokens to generate\n",
    "        temperature: The temperature to use for the model\n",
    "    \n",
    "    Outputs:\n",
    "        model_preds: A list of model predictions for each test prompt\n",
    "    \"\"\"\n",
    "\n",
    "    model_preds = []\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "        \n",
    "        \n",
    "    return model_preds\n",
    "\n",
    "def evaluate_model_preds(\n",
    "        model_preds,\n",
    "        test_labels\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates the prediction of the model by performing string match between the predictions and labels.\n",
    "\n",
    "    Inputs:\n",
    "        model_preds: A list of model predictions for each test prompt\n",
    "        test_labels: A list of test labels. Note that these are not verbalized\n",
    "\n",
    "    Outputs:\n",
    "        accuracy: The accuracy of the model i.e. #correct_predictions / #total_predictions\n",
    "    \"\"\"\n",
    "\n",
    "    accuracy = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    return accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test if things are working fine\n",
    "k = 5\n",
    "test_prompts = val_prompts[:k]\n",
    "test_labels = dev_labels[:k]\n",
    "model_preds = get_model_predictions(test_prompts, few_shot_examples,\n",
    "                                    model_name = \"llama3-70b-8192\",\n",
    "                                    max_tokens = 20, temperature = 0.0)\n",
    "\n",
    "accuracy = evaluate_model_preds(model_preds, test_labels)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on 32 validation examples\n",
    "k = 32\n",
    "test_prompts = val_prompts[:k]\n",
    "test_labels = dev_labels[:k]\n",
    "model_preds = get_model_predictions(test_prompts, few_shot_examples,\n",
    "                                    model_name = \"llama3-70b-8192\",\n",
    "                                    max_tokens = 20, temperature = 0.0)\n",
    "\n",
    "accuracy = evaluate_model_preds(model_preds, test_labels)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on 32 validation examples with explanations\n",
    "k = 32\n",
    "test_prompts = val_prompts[:k]\n",
    "test_labels = dev_labels[:k]\n",
    "model_output, model_preds = get_model_predictions(test_prompts, \n",
    "                                    fs_examples_w_explanations, \n",
    "                                    model_name = \"llama3-70b-8192\",\n",
    "                                    max_tokens = 150, temperature = 0.0)\n",
    "accuracy = evaluate_model_preds(model_preds, test_labels)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doing 128 examples will take some time of around 30 minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on 128 validation examples with explanations\n",
    "k = 128\n",
    "test_prompts = val_prompts[:k]\n",
    "test_labels = dev_labels[:k]\n",
    "model_output, model_preds = get_model_predictions(test_prompts, \n",
    "                                    fs_examples_w_explanations, \n",
    "                                    model_name = \"llama3-70b-8192\",\n",
    "                                    max_tokens = 150, temperature = 0.0)\n",
    "accuracy = evaluate_model_preds(model_preds, test_labels)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we get slightly better performance on prompting the model with explanations than without 78.9% vs 71.875%. We can do more prompt-engineering and better type of explanations to improve the performance further. Also, there may be instances where the answer was correct but our pattern matching didn't catch the correct answer and marked it incorrect.\n",
    "\n",
    "But we hope with this you would have gotten some idea on how to use these models to solve NLP tasks like this. Also, notice that common sense reasoning remains an open problem for the models we have today, as even with LLMs like LLaMa3, ChatGPT, which are fairly strong LLMs, the accuracy remains isn't as high as we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Now, lets move to another task, where our goal is to create a question answering system for students to ask questions about professors from Plaksha University. \n",
    "\n",
    "The data for professors has been scrapped from the university full-time faculty webpage and stored in a csv file. The csv file has the following columns:\n",
    "- name: The name of the professor\n",
    "- expertise: The area of expertise of the professor\n",
    "- interest: The research interests of the professor\n",
    "- about: A short bio/about of the professor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/Plaksha.csv\")\n",
    "df.fillna(\"N/A\", inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 Creating Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an embedding prompt by combing multiple columns which will be sent to the embedding model to get the final embedding.\n",
    "\n",
    "Template: `Professor <professor_name>'s Area of Expertise is in <expertise> and some of the professor's Research Interests are <interest>. Here is a short Bio/About of the Professor: <about>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_prompt(x):\n",
    "    \"\"\"\n",
    "    This function takes a row of the dataframe as input and returns a string.\n",
    "    The string is a combination of the professor's name, expertise, research interests, and bio.\n",
    "    \"\"\"\n",
    "    return f\"Professor {x['name']}'s Area of Expertise is in {x['expertise']} and some of the professor's Research Interests are {x['interest']}. Here is a short Bio/About of the Professor:\\n\\n {x['about']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the embedding prompt to the dataframe to create a new column `prompt` which will be used to get the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prompt'] = df.apply(embedding_prompt, axis=1)\n",
    "assert 'prompt' in df.columns, \"Prompt column not found\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting embeddings from the model `voyage-large-2-instruct` and save it as a new column `embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(x, input_type='document'):\n",
    "    \"\"\"\n",
    "    This function takes a prompt string as input and returns the embedding of the prompt.\n",
    "    The embedding is obtained using the voyage-large-2-instruct model.\n",
    "    \"\"\"\n",
    "    # time.sleep(25)\n",
    "    emb_obj = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return emb_obj.embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the embeddings dataset csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/Plaksha_with_embeddings.csv\")\n",
    "df.fillna(\"N/A\", inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the embedding column which is a string of list to a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2np(x):\n",
    "    \"\"\"\n",
    "    This function takes a string representation of a list as input, \n",
    "    removes the brackets, splits the string into a list, converts the elements of the list to float, \n",
    "    and finally converts the list to a numpy array.\n",
    "\n",
    "    HINT: Use the replace and split functions\n",
    "    HINT: Remove brackets from string, and using split to convert to normal list\n",
    "    HINT: Covert the elements of list to float\n",
    "    HINT: Convert list to numpy array\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return np.array(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the `str2np` function to the embedding column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embedding'] = df['embedding'].apply(str2np)\n",
    "\n",
    "assert 'embedding' in df.columns, \"Embedding column not found\"\n",
    "assert isinstance(df['embedding'][0], np.ndarray), \"Embedding column is not of type numpy array\"\n",
    "assert df['embedding'][0].shape == (1024,), \"Embedding column is not of shape (1024,)\"\n",
    "assert df['embedding'][0].dtype == np.float64, \"Embedding column is not of type float64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document In-Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a numpy 2D array from the embedding column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.stack(df['embedding'])\n",
    "assert embeddings.shape == (37, 1024), \"Incorrect Embedding Shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a cosine similarity function which will be used to get the similarity between the query and the professors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    This function calculates the cosine similarity between two vectors.\n",
    "\n",
    "    Parameters:\n",
    "    a (numpy array): The first vector.\n",
    "    b (numpy array): The second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity between the two vectors.\n",
    "    \"\"\"\n",
    "    similarity = None\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for cosine similarity\n",
    "\n",
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6]\n",
    "print(\"Your cosine similarity is: \", cosine_similarity(a, b))\n",
    "print(\"Expected cosine similarity is: \", 0.9746318461970762)\n",
    "assert np.isclose(cosine_similarity(a, b), 0.9746318461970762, atol=0.00001), \"Incorrect Cosine Similarity\"\n",
    "\n",
    "a = [0.1, 0.6, 0.8, 0.6, 0.34, 0.78, 0.65, 0.88, 0.1, 0.98, 0.34, 0.77]\n",
    "b = [0.8, 0.5, 0.44, 0.67, 0.4, 0.6, 0.7, 0.23, 0.87, 0.45, 0.78, 0.98]\n",
    "print(\"\\nYour cosine similarity is: \", cosine_similarity(a, b))\n",
    "print(\"Expected cosine similarity is: \", 0.7814329877768034)\n",
    "assert np.isclose(cosine_similarity(a, b), 0.7814329877768034, atol=0.00001), \"Incorrect Cosine Similarity\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the N most similar professors for a given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarProfessor(query, df, embeddings_matrix):\n",
    "    \"\"\"\n",
    "    This function returns the N most similar professors for a given query.\n",
    "    \n",
    "    Parameters:\n",
    "    query (str): The query for which similar professors are to be found.\n",
    "    df (pandas DataFrame): The DataFrame containing information about professors.\n",
    "    embeddings_matrix (numpy array): The matrix of embeddings for the professors.\n",
    "    \n",
    "    Returns:\n",
    "    pandas DataFrame: A DataFrame containing the N most similar professors.\n",
    "\n",
    "    # HINT: Get the embedding for the query first, input_type = 'query'\n",
    "    # HINT: Use the cosine_similarity function to get the similarity between the query and the professors.\n",
    "    # HINT: Use the np.argsort to get the indices of the most similar professors.\n",
    "    # IMPORTANT: argsort return indices in ascending order, but we need descending order.\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a system prompt which will do cosine similarity with the existing embeddings and get top N most similar professors for a given query.\n",
    "\n",
    "Template: `You are an helpful assistant, whose role is to help students with their queries. You will be given a context about one or more professors followed by a query from the student about which, what professor is better, or which topic does a particular professor is best at etc. Given the context you have to correctly answer the query and if there is not information in the context regarding the query then you have to answer 'No information available.'`\n",
    "\n",
    "You can play with the system prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are an helpful assistant, whose role is to help students with their queries. You will be given a context about one or more professors followed by a query from the student about which, what professor is better, or which topic does a particular professor is best at etc. Given the context you have to correctly answer the query and if there is not information in the context regarding the query then you have to answer 'No information available'.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for a given query, let's find the top N professor who are related to the query and create the context string for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_template(name, expertise, interest, about):\n",
    "    return f\"\"\"\n",
    "{name}:\n",
    "Area of Expertise: {expertise}\n",
    "The Professor's Research Interests are {interest}\n",
    "Here is the Bio/About the Professor:\n",
    "{about}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I want to be part of a change in Indian education, working with someone who is doing cutting edge research on education, who will be the best person?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = similarProfessor(query, df, embeddings)\n",
    "\n",
    "context = \"\"\n",
    "for index, row in result_df.iterrows():\n",
    "    context += context_template(row['name'], row['expertise'], row['interest'], row['about'])\n",
    "    context += \"\\n\\n\"\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model= \"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: \\n\\n {context}.\\n\\n Query: {query}\"}\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plaksha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
